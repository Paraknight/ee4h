\documentclass[a4paper,12pt,notitlepage]{article}
\usepackage{fullpage}
%\setkomafont{disposition}{\normalfont\bfseries}
%\usepackage{setspace}
%\setstretch{1.5}
\usepackage{graphicx}
\usepackage{float}
\usepackage{chngcntr}
\counterwithin{figure}{subsubsection}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
\usepackage[style=authoryear,maxcitenames=1,backend=biber]{biblatex}
\renewcommand*{\nameyeardelim}{\addcomma\addspace}
\usepackage[toc,page]{appendix}
\newcommand{\source}[2]{\emph{#1 } (appendix~\ref{#2}, page~\pageref{#2})}
\newcommand{\secref}[1]{(section~\ref{#1}, page~\pageref{#1})}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{color}
\newcommand{\code}[1]{\colorbox{white}{\lstinline[basicstyle=\ttfamily\color{black}]|#1|} }
\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0.7,1}
\usepackage{MnSymbol}
\lstset{
	language=C++,
	showspaces=false,
	showtabs=false,
	breaklines=true,
	breakatwhitespace=true,
	postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\rcurvearrowse\space}},
	breakautoindent=true,
	escapeinside={(*@}{@*)},
	commentstyle=\color{greencomments},
	keywordstyle=\color{bluekeywords},
	stringstyle=\color{redstrings},
	basicstyle=\ttfamily,
	captionpos=t,
	extendedchars=true,
	keepspaces=true,
	showstringspaces=false,
	stepnumber=2,
	tabsize=2,
}
\usepackage{hyperref}
\addbibresource{report.bib}
\graphicspath{{res/images/}}

\begin{document}

\parskip 2mm

\title{{\huge Playing Card Recognition}\\\vspace{2 mm}{\large \textbf{Computer Vision (EE4H) --- Final Report}}}
%\subtitle{EECE MEng4 FYP -– Second Report}
\author{Yousef Amar (1095307)\\Chris Lewis (1234567)}
\date{2014-04-28}
\maketitle
\thispagestyle{empty}
\vfill
%\begin{quotation}
\begin{abstract}
	Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
\end{abstract}
%\end{quotation}
\pagebreak

%\addtocontents{toc}{\protect\thispagestyle{empty}}
\tableofcontents
\thispagestyle{empty}
\pagebreak
\setcounter{page}{1}

\parskip 2mm

\section{Introduction}
	\subsection{Task Assigned}
		For the EE4H Computer Vision assignment the task set is to examine images of classic playing cards and develop an automated software solution for classifying their suit and rank without human intervention. To do this, an understanding of the existing techniques available to use must be gained, a software solution developed that meets the task requirements and that software must be evaluated and tested to ensure it is working correctly. 

		Therefore the task can be split into the following stages:

		\begin{enumerate}
			\item Image pre-processing to enhance features.
			\item Card isolation from input image.
			\item For each card:
			\begin{enumerate}
				\item Correct perspective
				\item Correct rotation
				\item Isolate suit symbol and rank symbol
				\item Find card rank (numerical value and picture card rank)
				\item Find card suit using suit symbol
			\end{enumerate}
			\item Display results of classification to user in easy to understand forms.
		\end{enumerate}

		In doing this the software must be able to compensate for issues that may hinder classification such as uneven lighting, different card orientations, single and multiple card images and card reflectivity.
	\subsection{Method Used}
		The chosen method to classify playing cards was determined after the review of literature and is as follows:
		\begin{itemize}
			\item Perform pre-processing to compensate for lighting.
			\item Isolate cards using their stark contours against the background.
			\item Correct isolated card perspective using a Perspective Transform.
			\item Correct card's rotation using corner intensities.
			\item Remove white background from card and use redness in corners to determine suit colour.
			\item Isolate rank symbol and suit symbol areas from isolated card corners.
			\item Use blob counting after morphological erosion and closing to determine rank in the case of value cards (2 -- 10), and morphological Hit-or-Miss in the case of picture cards.
			\item Present output classifications as a GUI cascade and system console textual output.
		\end{itemize}

		This method involves both high and low level computer vision and mathematical morphological techniques that are detailed in the Review section \secref{sec:review} before implementation as documented in the Implementation section \secref{sec:implementation}. Using this series of stages the software is able to compensate for many of the issues highlighted in the last section. However, it is designed for the classical playing card style, used the world over, and not for overly stylised, custom, heavily themed or obfuscated decks.
	% TODO: Is this a good heading?
	\subsection{Findings}
		% TODO: Figure with a capital F?
		After successful implementation of the above stages (and techniques employed therein) the software is able to correctly isolate [TODO: How successful?] cards in moderately favourable conditions. The software is also able to isolate and correctly transform cards at many orientations relative to the camera as well as discard playing cards that are face down. Once these cards have been isolated, each card's rank and symbol are correctly identified with a high rate of success [TODO: How high?]. This is illustrated in Figure~\ref{fig:success} below:

		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{\textwidth}
				\centering
				\includegraphics[width=0.8\textwidth]{chris/image1}
				\caption{}
			\end{subfigure}
		\end{figure}
		\begin{figure}[H]
			\ContinuedFloat
			\centering
			\begin{subfigure}[b]{\textwidth}
				\centering
				\includegraphics[width=0.8\textwidth]{chris/image2}
				\caption{}
			\end{subfigure}
		\end{figure}
		\begin{figure}[H]
			\ContinuedFloat
			\centering
			\begin{subfigure}[b]{\textwidth}
				\centering
				\includegraphics[width=0.8\textwidth]{chris/image3}
				\caption{}
			\end{subfigure}
			\caption{A set of playing cards (a) at various orientations with a visual output of successful classifications (b) (c)}
			\label{fig:success}
		\end{figure}

		However, when the input image is of low quality or there are too many cards per image (\textgreater $\approx$12) the classification is not always successful, but it has been found that in these situations is at least a small number are correctly identified, particularly if the unfavourable light varies across the image. Some examples of such situations are shown in Figure~\ref{fig:fail}:

		% TODO: Images with window borders from now on
		% TODO: Light distortion still an issue?
		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{\textwidth}
				\centering
				\includegraphics[width=0.8\textwidth]{chris/image4}
				\caption{}
			\end{subfigure}
		\end{figure}
		\begin{figure}[H]
			\ContinuedFloat
			\centering
			\begin{subfigure}[b]{\textwidth}
				\centering
				\includegraphics[width=0.8\textwidth]{chris/image5}
				\caption{}
			\end{subfigure}
			\caption{An example of miss-classification (b) due to significant lighting distortion (a)}
			\label{fig:fail}
		\end{figure}
\pagebreak
\section{Review}
	\subsection{Identify sub-tasks}
		(Flowhart breakdown from final PPT?)
	\subsection{Review of Potential Methods}
		(existing literature (HIPR2?))
		Should each include limitations if not chosen, justification in 3.2
		\subsubsection{High-level Isolation}
			Contours
			Perspective transform (That OpenGL citation?)
		\subsubsection{Low-level Classification}
			Once a card has been identified in the input image, its perspective transformed and isolated to a separate image, it must be classified according to suit and rank. In order to do this, techniques are available from the field of Mathematical Morphology that allow the modification of an image to enhance areas of interest as a form of pre-processing.

			In practice, these operators are implemented by passing a small group of pixels called a structuring element over the entirety of an input image, and a set of rules applied to the pixels present in the SE at each location in the image. Once the SE has been passed over the entire image, the operation is complete.

			The basic morphological operators available include \autocite{sonka1999image1}:

			\begin{itemize}
				\item Erosion --- For each image location, the output pixel is set to the lowest value within the SE.
				\item Dilation --- As for erosion, but the output becomes the highest neighbouring value. 
				\item Opening --- Combination of erosion followed by dilation. The result is an image with small areas of the foreground removed. Areas that completely house the SE are left untouched.
				\item Closing --- Similar to opening except areas smaller than the SE are filled in to the foreground.
			\end{itemize}

			All these operators can be defined mathematically using set theory. For an image matrix $X$ iterated over by a structuring element $B$, the result $Y$ of the dilation operation is the \emph{supremum}(greatest value) of the pixels in the input image under $B$ at pixel position $x$ in $X$.

			\begin{equation}
				X \oplus B = \{ Y \colon y = \sup⁡[B(x)], x \in X \}
			\end{equation}

			Similarly for erosion:

			\begin{equation}
				X \ominus B = \{ Y \colon y = \inf[B(x)], x \in X \}
			\end{equation}

			These two basic operators are then combined to produce the opening and closing operators, shown in the same terms below as the dilation of an erosion with the same sized structuring element \autocite{sonka1999image2}:

			\begin{equation}
				X \circ B = ( X \ominus B ) \oplus B
			\end{equation}

			Similarly for closing; the erosion of a dilated image:

			\begin{equation}
				X \bullet B = ( X \oplus B ) \ominus B
			\end{equation}

			Examples of the behaviour of these operators are shown in Figure~\ref{fig:morph} below:

			\begin{figure}[H]
				\centering
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image6}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image7}
					\caption{}
				\end{subfigure}\\
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image8}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image9}
					\caption{}
				\end{subfigure}\\
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image10}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image11}
					\caption{}
				\end{subfigure}
				\caption{An original image (a), after binary threshold (b). Further results of erosion (c), dilation (d), opening (e) and closing (f)}
				\label{fig:morph}
			\end{figure}

			These operators can be used to help clean up a segmented image and remove noise and spurious pixels to aid later stages of processing.
		\subsubsection{Hit-or-Miss Transform}
			The Hit-or-Miss Transform is another morphological technique that is useful as a form of binary pattern matching. This means it can be used to compare a sample image with a desired feature image to find instances in the sample image. In the context of classifying playing cards, this technique has potential to be used to enable the detection of the card's suit symbol as well as the rank symbol when compared to clean templates of each symbol type.

			The basis of operation is similar to the previous operators except that the structuring element passed over the input image is that of the pattern to be found. The structuring element is divided into two sets; the `hit' set and the `miss' set. When both sets are eroded, the result is an output point wherever the input image matches the structuring element. A mathematical description of the operation is shown below, and describes the result of the Transform to be the intersection of the hit and miss sets eroded by the image and image complement respectively \autocite{badea}:

			\begin{equation}
				X \otimes B = ( A \ominus C ) \cap ( A^C \ominus D )
			\end{equation}

			An example of the Hit-or-Miss Transform is shown below in Figure~\ref{fig:hom}:

			\begin{figure}[H]
				\centering
				\begin{subfigure}[b]{0.3\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image12}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.3\textwidth}
					\centering
					\includegraphics[]{chris/image13}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.3\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image14}
					\caption{}
				\end{subfigure}\\
				\begin{subfigure}[b]{0.3\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image15}
					\caption{}
				\end{subfigure}
				\caption{A sample image of symbols (a) processed with a structuring element (b) yields markers wherever there is a match (c). Overlaid on input image with large markers (d).}
				\label{fig:hom}
			\end{figure}

			Note in the above figure that no markers are present when the SE symbol is obstructed. This prevents a complete match. In this way it is possible to locate a pre-determined symbol inside a larger image by iterating over the entire matrix.
\pagebreak
\section{Proposed Method}
	\subsection{Logical progression from whole image to classification}
		Program flow, flowchart
	\subsection{Justify each stage’s techniques}
		Identify strengths and weaknesses
\pagebreak
\section{Implementation}
	Justify custom implementations vs. OpenCV pre-made function
	\^(interleaved and when relevant)
	\subsection{Image IO/Webcam IO}
		The loading of images to analyse from either disk or webcam is one of the most basic uses of the OpenCV library. To load in image from a file on a local or network hard disk drive, the \code{cv::imread()} function is called with the path to the image and read mode supplied as arguments. This is shown in the code segment below for an image path supplied as the second command line argument to be read as a colour image:

		\begin{lstlisting}

cv::Mat input = cv::imread(argv[1], CV_LOAD_IMAGE_COLOR);
		\end{lstlisting}

		Once this call has been made, the image matrix is checked for integrity by examining the width and height of the \code{cv::Mat} returned from \code{cv::imread()}:

		\begin{lstlisting}

cv::Size input_size = input.size();

//Check size is greater than zero
if(input_size.width > 0 && input_size.height > 0)
{
	...
}
		\end{lstlisting}

		If this check succeeds, the image is valid and is passed on to \code{process_image()} for further processing and eventual classification. Otherwise the program terminates with a suitable error message:
		
		\begin{lstlisting}

if (process_image(input))
	return EXIT_FAILURE;
		\end{lstlisting}

		If the `--cam' command line argument is supplied, the image is instead loaded from a webcam frame during live capture. A loop is entered displaying images from the webcam until the space bar is pressed, the loop is exited and the image processed as shown previously. This decision process is shown below:

		\begin{lstlisting}

bool from_cam = !strcmp(argv[1], "--cam");

...
//Open a webcam source
cv::VideoCapture cap(CV_CAP_ANY);
cv::waitKey(1000);

if(!cap.isOpened())
{
    cerr << "Unable to access webcam" << endl;
    return -3;  //Incorrect arguments code
}

cout << "Press space to take a photo" << endl;

char key;
do
{
    key = cvWaitKey(10);

    cap >> input;

    cv::imshow("Webcam", input);

    // Break out of loop if Space key is pressed
} while (char(key) != 32);
		\end{lstlisting}

		Once the isolation and classification of any cards present in the captured webcam frame is complete, the user can opt to capture another frame or exit the program.



	\subsection{Card Isolation}

		Once an image is passed in as an input, the first step is to find one or more cards in the image and, if any were found, extract them from the image and prepare them for processing. This is done in a number of steps.

		\subsubsection{Card Detection}

			In order to detect cards within the image, the function \code{find_cards()} in \source{isolation.cpp}{app:maincpp} is called from \source{main.cpp}{app:maincpp}. It takes in an empty vector of objects of type \code{Card} as an inputs which it then populates with the detected cards for later processing.

			Cards are detected by finding quadrilaterals with a method not unlike the one used in the sample file \emph{squares.cpp} in the official OpenCV repository \autocite{squares} albeit with some decisive distinctions. The backbone of the algorithm is the OpenCV function \code{cv::findContours()}. This function retrieves contours from a binary image using the algorithm developed by \textcite{suzuki1985topological} via border following. It was decided not to implement a custom function since it would not have added any flexibility or speed advantage over the OpenCV implementation. Indeed it would only have increased code complexity.

			% TODO: Label "High-level Isolation"
			This function requires a binary image as an input. As such, input images undergo a series of processing before hand. First, they are converted to greyscale. Then CLAHE --- as explained previously \secref{sec:isolation} --- is applied to these images. The size of the CLAHE window varies between 32x32 and 28x28 pixels depending on if the user has specified with the \code{--multi} command line flag that the program should run in multiple card mode. CLAHE operations using OpenCV functions run on the GPU through CUDA; they cannot be optimised by writing custom code for CLAHE without utilising CUDA, which would only introduce system-level bugs that have been long since resolved within OpenCV's code base. For this reason, the OpenCV CLAHE implementation was used.

			% TODO: IMAGES

			After performing CLAHE, the image is blurred. The amount of blur is set as a function of it image's dimensions. More specifically \code{(image_size.width + image_size.height)>>8} or in plain English, the perimeter of the image divided by 512. This prevents small images from losing too much detail through blurring while large images do not remain too sharp. Blurring is done to filter out noise for the next step.

			The resulting image is then converted to binary using the threshold parameter before \code{cv::findContours()} is called. Contours are found for processed binary images for thresholds ranging from 150 to 255 (both inclusive) in increments of 2. This bright range was experimentally determined to be the best to create binary images from.

			The found contours are then approximated into geometry using with the function \code{cv::approxPolyDP()} and then go through a number of rigorous checks. Contours that do not pass these checks are discarded and not added to the final contours collection that the function populates. These include:

			\begin{enumerate}
				\item Does the contour have four corners?\\[4px]
					Although card contours can have more than 4 approximated corners if the cards are bent or folded significantly, most will not, so the design does not handle card contours with more than four corners.
				\item Is its area greater than 4\% of the whole image?\\[4px]
					This is to filter out noise.
				\item Is it convex in shape like card borders?
				\item Is its area less than 75\% of the image?\\[4px]
					This is to filter out borders.
				% TODO: More detail? Including reason for fixing?
				\item Is it not inside another previously detected contour or another inside it?\\[4px]
					Calculated mathematically from vertex coordinate information. This is to filter out symbol (e.g. diamonds) and picture card misclassification. Previously detected contours in the list that turn out to be contained by newly detected contours are replaced by the newly detected and discarded.
				\item Is it significantly dissimilar to previously detected contours?\\[4px]
					The criteria for this check are that every rotation of the four contour vertices are at least a certain total distance away from their counterparts (summed up manhattan/city block distance in pixels). This minimum total distance is a function of the image size to normalise pixel units into units relative to the image dimensions. This relationship is \code{(image_size.width + image_size.height) * 0.5F * min_square_diff} or in other words, the mean image side length (to account for disproportionate image aspect ratios) multiplied by a constant factor set to $0.1$, i.e. 10\% of the mean image side length.
			\end{enumerate}

			Once all contours have been iterated through and the ones that fail these criteria have been filtered out, the result is a collection of contour quads that surround at least every card in the image (in the form of a \code{vector<vector<cv::Point> >} --- a vector of vectors with four points each). \emph{At least} every card in the image; the pruning process is not yet over!

			The assortment of contours go through a secondary, later filtration process that requires processing the pixel data within these contours. To do this --- as well as to construct the card Mats for subsequent classification --- the contour quad data is used to perspective transform the original image into a rectangular image of the card, an operation reviewed previously \secref{sec:isolation}.

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.6\linewidth]{perstrans1}
				\includegraphics[width=0.3\linewidth]{perstrans2}
				\caption{Output of Perspective Transform of Contour Quad from Test Image}
				\label{fig:perstrans}
			\end{figure}

			The perspective transform is low-level enough that its implementations do not vary much at all. The maths involved in calculating the 3x3 matrix of a perspective transform that can be used to map pixels at a perspective to other shapes/perspectives is standard but cumbersome to implement (unlike an affine transform). For these reasons, the OpenCV function \code{cv::getPerspectiveTransform()} was used. It takes two quads as inputs and returns the map matrix. The source quad is the detected contour and the destination quad is a simple 250x350 rectangle which matches the standard 5:7 poker card aspect ratio like in figure~\ref{fig:perstrans}.

			The transformation matrix is then used in the function \code{cv::warpPerspective()} which deforms the pixel grid of the source image and maps it to the destination image while interpolating transitional pixels; a process that is both non-trivial as well as distant from the task at hand. The Mat produced through this process is subsequently used to instantiate a new \code{Card} object.

			On instantiation, the \code{Card} constructor additionally generates an 8x8 CLAHE version and a 140 threshold binary version of the card Mat, which are publicly accessible from a \code{Card} object. Once instantiated, the cards binary Mat is finally used for the last phase of isolation filtering.

			The last criteria for what makes a card is its whiteness measure. The measure is simple; all non-zero (i.e. white) pixels in the card Mat are counted and the final value is divided by the card's area giving a float that describes the proportion of whiteness in the card. The whiteness threshold is set to an experimentally practical \code{0.5F} or 50\% bearing in mind that error due to lighting inconsistency is a non-issue following CLAHE. Cards that are not white enough are discarded. This solves false detections such as that in figure~\ref{fig:whiteness}.

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.9\linewidth]{whiteness}
				\caption{Example of Whiteness Check Ignoring Overturned Card}
				\label{fig:whiteness}
			\end{figure}

			Finally, there is one more processing step that needs to be undertaken before a detected card is deemed worthy enough to join its brethren in the detected card vector and \code{find_cards()} can return\ldots

		\subsubsection{Rotation Correction}

			One deal-breaking issue was the problem of a sequence of contour quad vertices starting at the wrong corner and, as a direct consequence, causing the final card Mat to be perspective transformed incorrectly. This problem became evident became evident when processing cards where the top-most corner in the image is numbered, such as the lower examples in figure~\ref{fig:rot}; the card could just be at a very strange perspective that projects to that shape.

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.6\linewidth]{rot1}
				\includegraphics[width=0.3\linewidth]{rot2}\\[5px]
				\includegraphics[width=0.6\linewidth]{rot3}
				\includegraphics[width=0.3\linewidth]{rot4}
				\caption{Example of Erroneous Perspective Transformation}
				\label{fig:rot}
			\end{figure}

			The solution developed is simple and works very well. The same measure of whiteness as is used for the whole card is applied to both the top left and the bottom right corners of the card Mat (the bounds for which are defined as static constants in the \code{Card} class based on physical measurements). If corner whiteness surpasses 80\%, the card quad is rotated by one vertex --- the direction or rotation being arbitrary due to a card's symmetry --- and transformed into a new Mat.

			The corner whiteness for this new Mat is also measured, and compared against old corner whiteness. If it has a lesser corner whiteness measure than the old, that means that the old card Mat was indeed oriented incorrectly and so the new one is used instead. If the opposite holds true, then the first overall whiteness threshold was passed incorrectly (perhaps due to some bizarre specular lighting that the CLAHE did not quite catch) and so the new mat is discarded and the old one's usage is continued as if nothing ever happened.

			At long last, the \code{Card} vector contains all detected cards --- to a high degree of accuracy --- ready for processing and classification by the rest of the program, and the function can return.

	\subsection{Colour Detection}
		The set of four traditional playing card suits are split into two further subcategories; diamonds and hearts in red, and clubs and spades in black. This information can be determined beforehand when classifying a card and used to reduce the suit symbol search space, as well as help increase detection accuracy. For example, when compared side by side, the hearts and spades suits both appear similar due to their curves and large surface area, with the spaces symbol appearing similar to a heart when viewed upside down.

		\begin{figure}[H]
			\centering
			\includegraphics[width=0.6\linewidth]{chris/image16}
			\caption{Hearts and Spades can look similar, highlighting the usefulness of colour information integration.}
			\label{fig:colours}
		\end{figure}

		% TODO: Also to divide HoM search space by 2
		Using this line of thinking, the top left and bottom right corners of the card are evaluated according to their `redness'. A card that has a large surface area of `redness' is likely to be either a diamonds card or a hearts card. The first stage in this step of classification is to make the background of the card completely black. The reason behind this is that a white pixel is as bright as it is due to a large red component, along with large green and blue components also. By transforming all white pixels above a threshold into black pixels, only the majority-red pixels will remain. This is implemented as shown below:
		
		\begin{lstlisting}

//For all pixels...
for(int y = 0; y < input_size.height; y++)
{
    for(int x = 0; x < input_size.width; x++)
    {
        //Get BGR values
        int b = in_data[(y)*input.step + (x)*input_channels + 0];
        int g = in_data[(y)*input.step + (x)*input_channels + 1];
        int r = in_data[(y)*input.step + (x)*input_channels + 2];

        //Set output pixel
        if(b > white_level && g > white_level && r > white_level)
        {
            //Make it black
            out_data[(y)*output.step + (x)*output_channels + 0] = 0;
            out_data[(y)*output.step + (x)*output_channels + 1] = 0;
            out_data[(y)*output.step + (x)*output_channels + 2] = 0;
        }
        else
        {
            //Leave pixel as it is
        }
    }
}
		\end{lstlisting}

		The result of this operation is illustrated below in Figure~\ref{fig:whitebg}:
		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\includegraphics[width=\textwidth]{chris/image17}
				\caption{}
			\end{subfigure}
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\includegraphics[width=\textwidth]{chris/image18}
				\caption{}
			\end{subfigure}
			\caption{An isolated card (a) with white background pixels removed (b).}
			\label{fig:whitebg}
		\end{figure}

		Once this is complete, the image is then filtered to remove all green and blue component values, leaving a purely red image:
		
		\begin{lstlisting}

//For all pixels...
for(int y = 0; y < input_size.height; y++)
{
    for(int x = 0; x < input_size.width; x++)
    {
        //Set output pixel              
        out_data[(y)*output.step + (x)*output_channels + 0] = new_value;    //B
        out_data[(y)*output.step + (x)*output_channels + 1] = new_value;    //G 
    }
}
		\end{lstlisting}

		where \code{new_value} is set to 0 (black). The red component is left untouched. The result is shown below:

		\begin{figure}[H]
			\centering
			\includegraphics[width=0.4\textwidth]{chris/image19}
			\caption{An isolated card (a) with white background pixels removed (b).}
			\label{fig:redchan}
		\end{figure}

		The final step in this suit colour processing stage is to evaluate whether the top-left and bottom-right corners are sufficiently high in red surface area that they can be classified as belonging to a red suit. This final step of implementation is shown below (abbreviated):

		\begin{lstlisting}

bool is_red_suit_by_corners(cv::Mat input, int base_threshold, int target_regions, float perc_red)
{
    ...

    //Get four corner regions
    int region_width = Card::TOP_CORNER_RECT.width;
    int region_height = Card::TOP_CORNER_RECT.height;

    //Create region matrices, top left, bottom right...
    cv::Mat regions[2] = {
        input(Card::TOP_CORNER_RECT),
        input(Card::BOTTOM_CORNER_RECT)
    };

    //For all regions...
    for(int i = 0; i < 2; i++)
    {
        //Reset totals
        red = 0; green = 0; blue = 0;

        //Pointer to data
        uchar *data = (uchar*)regions[i].data;

        //Get region channels once for speed
        int region_channels = regions[i].channels();

        //For all pixels...
        for(int y = 0; y < region_height; y++)
        {
            for(int x = 0; x < region_width; x++)
            {
                //Get values
                int b = data[(y)*regions[i].step + (x)*region_channels + 0];
                int g = data[(y)*regions[i].step + (x)*region_channels + 1];
                int r = data[(y)*regions[i].step + (x)*region_channels + 2];

                if(max(r, g, b) == r && r > base_threshold)
                {
                    red++;
                }
                else if(max(r, g, b) == g && g > base_threshold)
                {
                    green++;
                }
                else if(max(r, g, b) == b && b > base_threshold)
                {
                    blue++;
                }
                else {
                    //No action
                }
            }
        }

        //Compute result
        float total = (float)region_width * (float) region_height;
        float match_perc = (float) red / total;
        regions_red[i] = (red > blue && red > green) && match_perc > perc_red;
        
        ...
    }

    //Count number of red regions
    int count = 0;
    for(int c = 0; c < 2; c++)
    {
        if(regions_red[c] == true)
        {
            count++;
        }
    }

    ...

    return count >= target_regions;
}
		\end{lstlisting}

		Thus if both the examined regions are found to be majority red above the parametrised percentage specified, the function returns \code{true}, meaning the card suit is a red suit, either diamonds or hearts. The result structure is then assigned the appropriate colour enumeration constant.

		When called as part of card isolation, the call appears as below:

		\begin{lstlisting}

card->detected_colour = is_red_suit_by_corners(working, 100, 2, 0.10F) ? Card::RED : Card::BLACK;
		\end{lstlisting}


	\subsection{Type Detection}
	\subsection{Symbol/Rank symbol Isolation}
	\subsection{Number Detection}
		The rank of a card is determined by the numerical value in the top-left and bottom-right corners, and in the case of any card that is not a picture card, the quantity of suit symbols in the centre of the card itself. Thus, cards 2 through 10 are scored by their numerical value by the software implementation using the quantity of suit symbols.

		\subsubsection{Value cards}
			The first step in this classification stage is to convert the card image to a binary segmented form using simple thresholding, then by flood-filling the top-left and bottom-right corners, whose features are not used for value detection, as shown below:

			\begin{lstlisting}

cv::Mat temp = card->mat_bin.clone();

//Ignore the corners
cv::rectangle(temp, Card::TOP_CORNER_RECT, cv::Scalar(255), CV_FILLED);
cv::rectangle(temp, Card::BOTTOM_CORNER_RECT, cv::Scalar(255), CV_FILLED);
			\end{lstlisting}

			Once this is complete, the remaining blobs are morphologically eroded and closed to clear up the regions into single homogeneous blobs. This is done using custom morphological implementations which can be found in the source code under \source{cl\_own.cpp}{app:clowncpp}.

			\begin{lstlisting}

//Erode and close
temp = binary_operation(temp, MODE_BINARY_DILATION, 5);
temp = binary_closing(temp, 10);
			\end{lstlisting}

			The result is shown in Figure~\ref{fig:closing}:

			\begin{figure}[H]
				\centering
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image20}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image21}
					\caption{}
				\end{subfigure}
				\caption{Binary threshold segmented card with extraneous features removed}
				\label{fig:closing}
			\end{figure}

			After this, the remaining blobs are counted and filled, and is equal to the value of the card itself for the sample deck used for evaluation.

		\subsubsection{Picture cards}
			In the case of picture cards (except for the case of an Ace), there is to relationship between the number of blobs in the centre of the card and its value, such as a King. To get around this, the presence of a picture card is determined by the presence of an additional contour inside the isolated card, i.e.: there is a rectangle within the card rectangle, as illustrated below:

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.4\textwidth]{chris/image22}
				\caption{A picture card such as Jack, Queen or King will have an interior contour}
				\label{fig:innercont}
			\end{figure}

			If this additional internal contour is found, its rank is determined using the rank symbol; `J', `Q' or `K', using a morphological Hit-or-Miss implementation. The region containing the isolated rank symbol area is used as the subject of a Hit-or-Miss pattern matching search against pre-created templates (or Structuring Elements), shown in Figure~\ref{fig:picstruct}:

			\begin{figure}[H]
				\centering
				\begin{subfigure}[b]{0.14\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image23}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.15\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image24}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.14\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image25}
					\caption{}
				\end{subfigure}
				\caption{Pre-created Hit-or-Miss Structuring Elements for picture card rank evacuation}
				\label{fig:picstruct}
			\end{figure}

			When the pattern matching is carried out, both the isolated rank symbol area and the pre-created template are scaled to the same size before comparison. If the percentage of similar pixels exceeds a parametrised threshold, the card is classified to that rank. An example for the card shown in Figure~\ref{fig:queenstruct} is shown below:

			\begin{figure}[H]
				\centering
				\begin{subfigure}[b]{0.26\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image26}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.25\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image37}
					\caption{}
				\end{subfigure}
				\caption{Identically scaled isolated rank symbol (a) and template (b)}
				\label{fig:queenstruct}
			\end{figure}

			The percentage match between the three rank symbol templates shown in Figure~\ref{fig:picstruct} are output to the system console, and show a clear match for `Q' for the card in Figure~\ref{fig:innercont}:

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.6\textwidth]{chris/image27}
				\caption{Console output shows percentage match in clear favour of a Queen rank classification.}
				\label{fig:console}
			\end{figure}

			This entire process is shown in the implementation segment below (abbreviated):

			\begin{lstlisting}

//Load original SEs
cv::Mat se_symbols[3] = {
    cv::imread("res/symbols/scale_full/jack.png", CV_LOAD_IMAGE_GRAYSCALE),
    cv::imread("res/symbols/scale_full/queen.png", CV_LOAD_IMAGE_GRAYSCALE),
    cv::imread("res/symbols/scale_full/king.png", CV_LOAD_IMAGE_GRAYSCALE)
};

//Calculate new size
cv::Size size(100, 100);
cv::resize(mat_rank_1c, mat_rank_1c, size); //Should always be resized at runtime

//For reach symbol, resize SE
for(int j = 0; j < 3; j++)
{
    //Resize SE
    cv::resize(se_symbols[j], se_symbols[j], size); 
}

//Do matches using colour info integration
matches[JACK] += (int)round(hit_or_miss_score(mat_rank_1c, se_symbols[JACK]) * 100.0F);
matches[QUEEN] += (int)round(hit_or_miss_score(mat_rank_1c, se_symbols[QUEEN]) * 100.0F);
matches[KING] += (int)round(hit_or_miss_score(mat_rank_1c, se_symbols[KING]) * 100.0F);

cv::imshow("input", mat_rank_1c);
cv::imshow("template", se_symbols[QUEEN]);  //Only relevant for threecards.jpg

cout << "Scores (J/Q/K): " << matches[JACK] << "/" << matches[QUEEN] << "/" << matches[KING] << endl;

//Find which suit was matched most
if(max(matches[JACK], matches[QUEEN], matches[KING]) == matches[JACK])
{
    cout << "Rank may be JACK!" << endl;
    card->detected_rank = Card::RANK_JACK;
}
else if(max(matches[QUEEN], matches[QUEEN], matches[KING]) == matches[QUEEN])
{
    cout << "Rank may be QUEEN!" << endl;
    card->detected_rank = Card::RANK_QUEEN;
}
else if(max(matches[KING], matches[QUEEN], matches[KING]) == matches[KING])
{
    cout << "Rank may be KING!" << endl;
    card->detected_rank = Card::RANK_KING;
}
else
{
    cout << "No winner! UNKNOWN SUIT" << endl;
    card->detected_suit = Card::UNKNOWN_SUIT;
}
			\end{lstlisting}

		\subsubsection{Custom HoM Implementation}
			Mathematical Morphology operations are available as part of the OpenCV library (\code{cv::morphologyEx()}), but as a result of study of the theory and experimental requirements it was felt that a custom implementation of the theory was appropriate. This allows additional modification and parameterisation of the behaviour of required morphological operators. To this end the implementation uses the \code{binary_operation()}, \code{binary_closing()} and \code{hit_or_miss()} functions whose core workings are detailed below:

			A binary morphological erosion or dilation is performed on a pixel by pixel basis, with each pixel in the input image used as the centre of a symmetrical structuring element. In the case of erosion, the lowest value (any 0s) is set at the output pixel. In the case of dilation the greatest value (any 255s) is set as the output pixel. This is shown in the code segment below:

			\begin{lstlisting}

//For all X x Y pixels
int result = 0;
for(int y = 0; y < input.rows; y++)
{
    for(int x = 0; x < input.cols; x++)
    {
        //Reset for new neighbourhood
        result = 0;
            
        //Get local values
        bool break_now = false;
        for(int j = 0; j < element_size; j++)
        {
            for(int i = 0; i < element_size; i++)
            {
                //If in the image
                if(is_in_image((x - half) + i, (y - half) + j, input.cols, input.rows))
                {
                    //Channel values
                    int p = data[((y - half) + j) * input.step + ((x - half) + i) * input_channels];

                    //Save brightest of that area
                    if(mode == MODE_BINARY_DILATION)
                    {
                        if(p  > 128.0F) //Assuming binary input
                        {
                            result = 255;
                            break_now = true;
                            break;
                        }
                    }
                    else
                    {
                        if(p < 128.0F)
                        {
                            result = 0;
                            break_now = true;
                            break;
                        }
                        else
                        {
                            result = 255;
                        }
                    }
                }
            }

            if(break_now == true)   //This is used for speed. Eg for erosion, if any are black, all become black
            {
                break;
            }
        }
    
        //Save output Mat data
        out_data[y * output.step + x * output_channels] = result;
    }
}
			\end{lstlisting}

			The end result of this operation is a \code{cv::Mat} containing an eroded or dilated image, depending on the mode passed as an argument. 

			The \code{binary_opening()} and \code{binary_closing()} functions simply chain calls to \code{binary_operation()} to implement the order of operations required for each operator. The example of \code{binary_closing()} is shown below:

			\begin{lstlisting}

cv::Mat binary_closing(cv::Mat input, int element_size)
{
    cout << "Performing binary closing..." << endl;

    cv::Mat working = input.clone();
    working = binary_operation(working, MODE_BINARY_DILATION, element_size);
    working = binary_operation(working, MODE_BINARY_EROSION, element_size);
    return working;
}
			\end{lstlisting}

			Finally, the morphological Hit-or-Miss transform is implemented by comparing all pixels in an input image to those in the equivalent locations in the structuring element image. Once all pixels have been compared, the total number of matched pixels is compared against a minimum to count as a match. In the case of \code{hit_or_miss_score()} the raw percentage of matched pixels is output as the return value. The implementation in code is shown below:

			\begin{lstlisting}

//For each position, try and find a match
int matched_pixels = 0;

//Search whole SE
for(int j = 0; j < img.rows; j++)
{
    for(int i = 0; i < img.cols; i++)
    {
        //Input pixel value
        int in_p = in_data[j * img.step + i * in_channels];
        int se_p = se_data[j * img.step + i * se_channels];

        //Minithresh - but should be binary anyway!
        in_p = in_p > 128 ? 255 : 0;
        se_p = se_p > 128 ? 255 : 0;

        if(in_p == se_p)
        {
            //Matching pixel found!
            matched_pixels++;
        }
    }
}

//Was this a matched template?
int total_pixels = img.rows * img.cols;
return (float)matched_pixels / (float)total_pixels;
			\end{lstlisting}

			This return value is then used to compare multiple structuring elements in the picture card rank finding as well as the suit symbol detection functions.

	\subsection{Suit Symbol Detection}
		Once a card has been isolated from an image and its perspective correctly transformed, the isolated suit symbol in the top-left corner is compared to a set of pre-created symbols using the previously discussed Hit-or-Miss Transform.

		This operation is completed in two stages. First, the suit colour is determined, then the symbol is pattern matched. The suit search space is halved by the suit colour evaluation and also aids detection accuracy by defining pairs of suits as mutually exclusive in the detection. For example, a hearts symbol cannot be miss-classified as a spaces symbol once the colour of the suit is determined.

		\begin{figure}[H]
			\centering
			\includegraphics[width=0.95\textwidth]{chris/image38}
			\caption{Flow diagram of suit symbol classification showing mutually exclusive possibilities based on detected colour.}
		\end{figure}

		This decision is shown in the code segment below:

		\begin{lstlisting}

//Do matches using colour info integration
if(card->detected_colour == Card::RED)
{
    matches[DIAMOND] += (int)round(hit_or_miss_score(mat_sym_1c, se_symbols[DIAMOND]) * 100.0F);
    matches[HEART] += (int)round(hit_or_miss_score(mat_sym_1c, se_symbols[HEART]) * 100.0F);
}
else if(card->detected_colour == Card::BLACK)
{
    matches[CLUB] += (int)round(hit_or_miss_score(mat_sym_1c, se_symbols[CLUB]) * 100.0F);
    matches[SPADE] += (int)round(hit_or_miss_score(mat_sym_1c, se_symbols[SPADE]) * 100.0F);
}
else
{
    cout << "Colour has not been detected before find_suit_sym()!" << endl;
    return;
}
		\end{lstlisting}

		The second stage is performed with the Hit-or-Miss Transform applied to the isolated symbol \code{cv::Mat} using each of four pre-created symbol images, shown in Figure~\ref{fig:structelems}, after being resized to the same dimensions.

		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{0.15\textwidth}
				\centering
				\includegraphics[width=\textwidth]{chris/image28}
				\caption{}
			\end{subfigure}
			\begin{subfigure}[b]{0.15\textwidth}
				\centering
				\includegraphics[width=\textwidth]{chris/image29}
				\caption{}
			\end{subfigure}
			\begin{subfigure}[b]{0.15\textwidth}
				\centering
				\includegraphics[width=\textwidth]{chris/image30}
				\caption{}
			\end{subfigure}
			\begin{subfigure}[b]{0.15\textwidth}
				\centering
				\includegraphics[width=\textwidth]{chris/image31}
				\caption{}
			\end{subfigure}
			\caption{The four pre-created suit symbol structuring elements.}
			\label{fig:structelems}
		\end{figure}

		Once both of the two applicable coloured structuring element images have been compared, the winning percentage match is then attributed in the structure for that card:

		\begin{lstlisting}

//Find which suit was matched most
if(max(matches[CLUB], matches[DIAMOND], matches[HEART], matches[SPADE]) == matches[CLUB])
{
    cout << "Suit may be CLUBS!" << endl;
    card->detected_suit = Card::CLUBS;  
}
else if(max(matches[CLUB], matches[DIAMOND], matches[HEART], matches[SPADE]) == matches[DIAMOND])
{
    cout << "Suit may be DIAMONDS!" << endl;
    card->detected_suit = Card::DIAMONDS;
}
else if(max(matches[CLUB], matches[DIAMOND], matches[HEART], matches[SPADE]) == matches[HEART])
{
    cout << "Suit may be HEARTS!" << endl;
    card->detected_suit = Card::HEARTS;
}
else if(max(matches[CLUB], matches[DIAMOND], matches[HEART], matches[SPADE]) == matches[SPADE])
{
    cout << "Suit may be SPADES!" << endl;
    card->detected_suit = Card::SPADES;
}
else
{
    cout << "No winner! UNKNOWN SUIT" << endl;
    card->detected_suit = Card::UNKNOWN_SUIT;
}
		\end{lstlisting}

		This matching is shown visually in the figure below in the case of a hearts card:

		\begin{figure}[H]
			\centering
			\includegraphics[width=0.6\textwidth]{chris/image32}
			\caption{Isolated and segmented card suit symbol and pre-created template used for comparison.}
		\end{figure}

		In this way the suit symbol is identified as contributes to the classification of the card in question.

	\subsection{Results output}
		Once all prior processes have been completed, the results of classification are output in two forms, as a GUI and on the system console. The GUI output is illustrated in the figure below:
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.4\textwidth]{chris/image33}
			\caption{Card classification GUI output showing card isolated (a), isolated suit symbol (b), isolated rank symbol (c) and textual results output (d).}
		\end{figure}

		For each detected card in the input image, the classified suit colour, suit symbol, value (0 for picture cards) and the rank detected (N/A for value cards).
		
		In this method of output it is easy to determine which elements of classification have succeeded or why they have failed when they do. For example, the isolated suit and rank symbol image matrices are reproduced at the bottom of the window, and therefore in the event of a problem in classification the intermediary isolations can be checked for errors. 
		
		The second form of output is shown in the form of a natural language result in the system console. A string is concatenated using all the fields stored in the Card structure once classification is complete to form a single plain-English sentence. The code segment below illustrates how this is done:

		\begin{lstlisting}

//Nice CMD summary
cout << endl << "============== Detected Cards ==============" << endl << endl;

for(size_t i = 0; i < cards.size(); i++)
{
    cout << "Card " << (i + 1) << ": The ";

    if(cards[i].is_picture_card)
    {
        switch(cards[i].detected_rank)
        {
        case Card::RANK_JACK:
            cout << "Jack";
            break;
        case Card::RANK_QUEEN:
            cout << "Queen";
            break;
        case Card::RANK_KING:
            cout << "King";
            break;
        case Card::RANK_ACE:
            cout << "Ace";
            break;
        default:
            cout << "Something";
            break;
        }
    }
    else
    {
        cout << cards[i].detected_value;
    }

    cout << " of ";

    switch(cards[i].detected_suit)
    {
    case Card::CLUBS:
        cout << "Clubs";
        break;
    case Card::DIAMONDS:
        cout << "Diamonds";
        break;
    case Card::HEARTS:
        cout << "Hearts";
        break;
    case Card::SPADES:
        cout << "Spades";
        break;
    default:
        cout << "Something";
        break;
    }

    cout << endl;
}

cout << endl << "============================================" << endl << endl;
		\end{lstlisting}

		This is repeated for each card in the cascade, and is shown below for an example cascade of classified cards:

		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{\textwidth}
				\centering
				\includegraphics[width=\textwidth]{chris/image34}
				\caption{}
			\end{subfigure}
		\end{figure}
		\begin{figure}[H]
			\ContinuedFloat
			\centering
			\begin{subfigure}[b]{\textwidth}
				\centering
				\includegraphics[width=\textwidth]{chris/image35}
				\caption{}
			\end{subfigure}
			\caption{Example correct classification cascade of six cards showing GUI (a) and console output (b) formats.}
		\end{figure}

		A criterion for improvement for future expansion or interoperability of the software with other systems could be a CSV (comma separated values) output or network connectivity, both of which are not too difficult to implement.

\pagebreak
\section{Evaluation}
	\subsection{Test images, increasing complexity}
	\subsection{Webcam tests}
	\subsection{Confusion Matrix}
\pagebreak
\section{Conclusion}
	\subsection{Summary}
	\subsection{Improvements}
\pagebreak
\thispagestyle{empty}
\printbibliography
\pagebreak
\begin{appendices}
	\section{Contribution Agreement}
		\begin{figure}[H]
			\centering
			\includegraphics[width=\textwidth]{chris/image36}
		\end{figure}
	\pagebreak
	\section{Source Code Listings}
		\lstset{
			language=C++,
			showspaces=false,
			showtabs=false,
			breaklines=true,
			breakatwhitespace=true,
			escapeinside={(*@}{@*)},
			commentstyle=\color{greencomments},
			keywordstyle=\color{bluekeywords},
			stringstyle=\color{redstrings},
			basicstyle=\ttfamily\scriptsize,
			captionpos=t,
			extendedchars=true,
			frame=single,
			keepspaces=true,
			showstringspaces=false,
			stepnumber=2,
			tabsize=2,
		}

		\subsection{main.cpp}
		\label{app:maincpp}
		\lstinputlisting{res/appendices/src/main.cpp}
\end{appendices}

\vfill
\hrulefill

\end{document}

\section{Introduction}
	
	This is a paragraph. Here's an example of cross-referencing: please see Review (section \ref{sec:review} on page \pageref{sec:review}).

	\subsection{Project Specification}

		This is a \emph{subsection}. Whitespace is mostly meaningless in \LaTeX. Indenting with tabs is just useful for collapsing parts of the document you aren't working on in Sublime; it has no effect on document appearance. Speaking of Sublime, these are some useful packages for working in \LaTeX:

		\begin{enumerate}
			\item ``LaTeXTools'' (better than ``LaTeXing'') because
			\begin{enumerate}
				\item Doesn't pester you to buy it
				\item Has pretty good syntax highlighting and auto completion
				\item Auto completion for cross-referencing figures/sections/appendices and citations too
				\item Includes LaTeX build system --- Ctrl+B to compile; no command line headaches
			\end{enumerate}
			\item Edit your settings file for really smooth spell check: \url{http://www.sublimetext.com/docs/3/spell_checking.html}
			\item ``Origami'' for multiple panes in Sublime --- useful to have notes open as a clipboard on the side
			\item ``Increment Selection'' (came in handy once or twice)
		\end{enumerate}

\pagebreak

\section{Review}
\label{sec:review}

	Here's another section. Here's a figure with 0.8 of the line width:

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\linewidth]{mixed}
		\caption{Example of Card}
		\label{fig:mixed}
	\end{figure}

	\LaTeX is pretty good at figuring out what to do with figure. There are a number of options for them too that are cool. Here's me cross-referencing the figure without breaking a sweat: please see figure \ref{fig:mixed} on page \pageref{fig:mixed}.

\pagebreak

\section{Proposed Method}

	\LaTeX is really good at equations. Here's a simple one from my report:

	$$x = \frac{ct}{2} - \epsilon$$

	Want some confusion matrices? Here are some matrix examples from my final report:

	\begin{equation*}
		T =
		\begin{bmatrix}
			84 & 200 & 16 \\
			56 & 238 & 11 \\
			251 & 206 & 203
		\end{bmatrix}
		\qquad
		I =
		\begin{bmatrix}
			160 & 40 & 250 & 27 & 114 \\
			81 & 94 & 14 & 100 & 37 \\
			225 & 52 & 148 & 137 & 111
		\end{bmatrix}
	\end{equation*}

	Here's me sprinkling some math inline: $\theta$, $x$ and $y$, $I_1$ and $I_2$, $|T - I_n|$. Want some graphs? Google pgfplots and prepare to be blown away.

\pagebreak

\section{Implementation}
	
	You can also put a lot of stuff in one figure even on different lines:
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.2\linewidth]{cards/jackofclubs}
		\includegraphics[width=0.2\linewidth]{cards/kingofdiamonds}
		\includegraphics[width=0.2\linewidth]{cards/queenofhearts}\\[5px]
		\includegraphics[width=0.3\linewidth]{cards/2ofdiamonds}
		\includegraphics[width=0.3\linewidth]{cards/3ofclubs}
		\caption{Example of Cards}
		\label{fig:stuff}
	\end{figure}

\pagebreak

\section{Evaluation}
	
	See appendix \ref{app:maincpp} on page \pageref{app:maincpp} for an example of code listings. The style is completely customisable of course. Notice also that it's not copied to the \emph{appendices} directory, it's just a symlink to \emph{src}! According to this random book off of Google Scholar, computer vision is pretty cool \autocite{forsyth2002computer}. I use BibLatex + Biber to compile the bibliography; the order is ``latexmk'' (or build), ``biber report'', then ``latexmk'' again. This only needs to be done if the bibliography changes and then it might as well be done only really at the very end when the report is finished to so don't worry about it. LaTeXing does it automatically but it's not worth it.

\pagebreak

\section{Conclusion}

	We've only just scratched the surface! Here, have a mini flow chart:

	% Define block styles
	\tikzstyle{decision} = [diamond, draw, fill=blue!20, text badly centered, inner sep=0pt, text width=4em, font=\tiny]
	\tikzstyle{process} = [rectangle, draw, fill=blue!20, text centered, rounded corners, text width=3em, font=\tiny]
	\tikzstyle{line} = [draw, -latex', font=\tiny]
	\tikzstyle{cloud} = [draw, ellipse, fill=red!20, minimum height=2em]
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[node distance = 2cm, auto]
			% Place nodes
			\node [process] (imgget) {Get Image from File/Webcam};
			\node [process, right of=imgget] (cardfind) {Isolate Cards};
			\node [process, right of=cardfind] (colourdet) {Detect Colour};
			\node [process, right of=colourdet] (typedet) {Detect Type};
			\node [process, right of=typedet] (symfind) {Isolate Symbols};
			\node [decision, right of=symfind] (ispic) {Is Card Picture Type?};
			\node [process, above right of=ispic] (numdet) {Detect Number};
			\node [process, below right of=ispic] (rankdet) {Detect Rank};
			% Enhanced by colour detection
			\node [process, below right of=numdet] (symdet) {Detect Suit};


			% Draw edges
			\path [line] (imgget) -- (cardfind);
			\path [line] (cardfind) -- (colourdet);
			\path [line] (colourdet) -- (typedet);
			\path [line] (typedet) -- (symfind);
			\path [line] (symfind) -- (ispic);
			\path [line] (ispic) -- node {no} (numdet);
			\path [line] (ispic) -- node [below left] {yes} (rankdet);
			\path [line] (rankdet) -- (symdet);
			\path [line] (numdet) -- (symdet);
			%\path [line,dashed] (expert) -- (init);
			%\path [line,dashed] (system) -- (init);
			%\path [line,dashed] (system) |- (evaluate);
		\end{tikzpicture}
		\caption{Top-level Flowchart}
		\label{fig:flowchart}
	\end{figure}


\end{document}