\documentclass[a4paper,12pt,notitlepage]{article}
\usepackage{fullpage}
%\setkomafont{disposition}{\normalfont\bfseries}
%\usepackage{setspace}
%\setstretch{1.5}
\usepackage{graphicx}
\usepackage{float}
\usepackage{chngcntr}
\counterwithin{figure}{subsubsection}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
\usepackage[style=authoryear,maxcitenames=1,backend=biber]{biblatex}
\renewcommand*{\nameyeardelim}{\addcomma\addspace}
\usepackage[toc,page]{appendix}
\newcommand{\source}[2]{\emph{#1 } (appendix~\ref{#2}, page~\pageref{#2})}
\newcommand{\secref}[1]{(section~\ref{#1}, page~\pageref{#1})}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{color}
\newcommand{\code}[1]{\colorbox{white}{\lstinline[basicstyle=\ttfamily\color{black}]|#1|} }
\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0.7,1}
\usepackage{MnSymbol}
\lstset{
	language=C++,
	showspaces=false,
	showtabs=false,
	breaklines=true,
	breakatwhitespace=true,
	postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\rcurvearrowse\space}},
	breakautoindent=true,
	escapeinside={(*@}{@*)},
	commentstyle=\color{greencomments},
	keywordstyle=\color{bluekeywords},
	stringstyle=\color{redstrings},
	basicstyle=\ttfamily,
	captionpos=t,
	extendedchars=true,
	keepspaces=true,
	showstringspaces=false,
	stepnumber=2,
	tabsize=2,
}
\usepackage{hyperref}
\addbibresource{report.bib}
\graphicspath{{res/images/}}

\begin{document}

\parskip 2mm

\title{{\huge Playing Card Recognition}\\\vspace{2 mm}{\large \textbf{Computer Vision (EE4H) --- Final Report}}}
%\subtitle{EECE MEng4 FYP -– Second Report}
\author{Yousef Amar (1095307)\\Chris Lewis (1234567)}
\date{2014-04-28}
\maketitle
\thispagestyle{empty}
\vfill
%\begin{quotation}
\begin{abstract}
	Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
\end{abstract}
%\end{quotation}
\pagebreak

%\addtocontents{toc}{\protect\thispagestyle{empty}}
\tableofcontents
\thispagestyle{empty}
\pagebreak
\setcounter{page}{1}

\parskip 2mm

\section{Introduction}
	\input{intro.tex}

\pagebreak
\section{Review}
	\subsection{Identify sub-tasks}
		(Flowhart breakdown from final PPT?)
	\subsection{Review of Potential Methods}
		(existing literature (HIPR2?))
		Should each include limitations if not chosen, justification in 3.2
		\subsubsection{High-level Isolation}
			Contours
			Perspective transform (That OpenGL citation?)
		\subsubsection{Low-level Classification}
			Once a card has been identified in the input image, its perspective transformed and isolated to a separate image, it must be classified according to suit and rank. In order to do this, techniques are available from the field of Mathematical Morphology that allow the modification of an image to enhance areas of interest as a form of pre-processing.

			In practice, these operators are implemented by passing a small group of pixels called a structuring element over the entirety of an input image, and a set of rules applied to the pixels present in the SE at each location in the image. Once the SE has been passed over the entire image, the operation is complete.

			The basic morphological operators available include \autocite{sonka1999image1}:

			\begin{itemize}
				\item Erosion --- For each image location, the output pixel is set to the lowest value within the SE.
				\item Dilation --- As for erosion, but the output becomes the highest neighbouring value. 
				\item Opening --- Combination of erosion followed by dilation. The result is an image with small areas of the foreground removed. Areas that completely house the SE are left untouched.
				\item Closing --- Similar to opening except areas smaller than the SE are filled in to the foreground.
			\end{itemize}

			All these operators can be defined mathematically using set theory. For an image matrix $X$ iterated over by a structuring element $B$, the result $Y$ of the dilation operation is the \emph{supremum}(greatest value) of the pixels in the input image under $B$ at pixel position $x$ in $X$.

			\begin{equation}
				X \oplus B = \{ Y \colon y = \sup⁡[B(x)], x \in X \}
			\end{equation}

			Similarly for erosion:

			\begin{equation}
				X \ominus B = \{ Y \colon y = \inf[B(x)], x \in X \}
			\end{equation}

			These two basic operators are then combined to produce the opening and closing operators, shown in the same terms below as the dilation of an erosion with the same sized structuring element \autocite{sonka1999image2}:

			\begin{equation}
				X \circ B = ( X \ominus B ) \oplus B
			\end{equation}

			Similarly for closing; the erosion of a dilated image:

			\begin{equation}
				X \bullet B = ( X \oplus B ) \ominus B
			\end{equation}

			Examples of the behaviour of these operators are shown in Figure~\ref{fig:morph} below:

			\begin{figure}[H]
				\centering
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image6}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image7}
					\caption{}
				\end{subfigure}\\
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image8}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image9}
					\caption{}
				\end{subfigure}\\
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image10}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.4\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image11}
					\caption{}
				\end{subfigure}
				\caption{An original image (a), after binary threshold (b). Further results of erosion (c), dilation (d), opening (e) and closing (f)}
				\label{fig:morph}
			\end{figure}

			These operators can be used to help clean up a segmented image and remove noise and spurious pixels to aid later stages of processing.
		\subsubsection{Hit-or-Miss Transform}
			The Hit-or-Miss Transform is another morphological technique that is useful as a form of binary pattern matching. This means it can be used to compare a sample image with a desired feature image to find instances in the sample image. In the context of classifying playing cards, this technique has potential to be used to enable the detection of the card's suit symbol as well as the rank symbol when compared to clean templates of each symbol type.

			The basis of operation is similar to the previous operators except that the structuring element passed over the input image is that of the pattern to be found. The structuring element is divided into two sets; the `hit' set and the `miss' set. When both sets are eroded, the result is an output point wherever the input image matches the structuring element. A mathematical description of the operation is shown below, and describes the result of the Transform to be the intersection of the hit and miss sets eroded by the image and image complement respectively \autocite{badea}:

			\begin{equation}
				X \otimes B = ( A \ominus C ) \cap ( A^C \ominus D )
			\end{equation}

			An example of the Hit-or-Miss Transform is shown below in Figure~\ref{fig:hom}:

			\begin{figure}[H]
				\centering
				\begin{subfigure}[b]{0.3\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image12}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.3\textwidth}
					\centering
					\includegraphics[]{chris/image13}
					\caption{}
				\end{subfigure}
				\begin{subfigure}[b]{0.3\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image14}
					\caption{}
				\end{subfigure}\\
				\begin{subfigure}[b]{0.3\textwidth}
					\centering
					\includegraphics[width=\textwidth]{chris/image15}
					\caption{}
				\end{subfigure}
				\caption{A sample image of symbols (a) processed with a structuring element (b) yields markers wherever there is a match (c). Overlaid on input image with large markers (d).}
				\label{fig:hom}
			\end{figure}

			Note in the above figure that no markers are present when the SE symbol is obstructed. This prevents a complete match. In this way it is possible to locate a pre-determined symbol inside a larger image by iterating over the entire matrix.
\pagebreak
\section{Proposed Method}
	\subsection{Logical progression from whole image to classification}
		Program flow, flowchart
	\subsection{Justify each stage’s techniques}
		Identify strengths and weaknesses
\pagebreak
\section{Implementation}
	Justify custom implementations vs. OpenCV pre-made function
	\^(interleaved and when relevant)
	\subsection{Image IO/Webcam IO}
		\input{imagewebcamio.tex}

	\subsection{Card Isolation}

		Once an image is passed in as an input, the first step is to find one or more cards in the image and, if any were found, extract them from the image and prepare them for processing. This is done in a number of steps.

		\subsubsection{Card Detection}

			In order to detect cards within the image, the function \code{find_cards()} in \source{isolation.cpp}{app:maincpp} is called from \source{main.cpp}{app:maincpp}. It takes in an empty vector of objects of type \code{Card} as an inputs which it then populates with the detected cards for later processing.

			Cards are detected by finding quadrilaterals with a method not unlike the one used in the sample file \emph{squares.cpp} in the official OpenCV repository \autocite{squares} albeit with some decisive distinctions. The backbone of the algorithm is the OpenCV function \code{cv::findContours()}. This function retrieves contours from a binary image using the algorithm developed by \textcite{suzuki1985topological} via border following. It was decided not to implement a custom function since it would not have added any flexibility or speed advantage over the OpenCV implementation. Indeed it would only have increased code complexity.

			% TODO: Label "High-level Isolation"
			This function requires a binary image as an input. As such, input images undergo a series of processing before hand. First, they are converted to greyscale. Then CLAHE --- as explained previously \secref{sec:isolation} --- is applied to these images. The size of the CLAHE window varies between 32x32 and 28x28 pixels depending on if the user has specified with the \code{--multi} command line flag that the program should run in multiple card mode. CLAHE operations using OpenCV functions run on the GPU through CUDA; they cannot be optimised by writing custom code for CLAHE without utilising CUDA, which would only introduce system-level bugs that have been long since resolved within OpenCV's code base. For this reason, the OpenCV CLAHE implementation was used.

			% TODO: IMAGES

			After performing CLAHE, the image is blurred. The amount of blur is set as a function of it image's dimensions. More specifically \code{(image_size.width + image_size.height)>>8} or in plain English, the perimeter of the image divided by 512. This prevents small images from losing too much detail through blurring while large images do not remain too sharp. Blurring is done to filter out noise for the next step.

			The resulting image is then converted to binary using the threshold parameter before \code{cv::findContours()} is called. Contours are found for processed binary images for thresholds ranging from 150 to 255 (both inclusive) in increments of 2. This bright range was experimentally determined to be the best to create binary images from.

			The found contours are then approximated into geometry using with the function \code{cv::approxPolyDP()} and then go through a number of rigorous checks. Contours that do not pass these checks are discarded and not added to the final contours collection that the function populates. These include:

			\begin{enumerate}
				\item Does the contour have four corners?\\[4px]
					Although card contours can have more than 4 approximated corners if the cards are bent or folded significantly, most will not, so the design does not handle card contours with more than four corners.
				\item Is its area greater than 4\% of the whole image?\\[4px]
					This is to filter out noise.
				\item Is it convex in shape like card borders?
				\item Is its area less than 75\% of the image?\\[4px]
					This is to filter out borders.
				% TODO: More detail? Including reason for fixing?
				\item Is it not inside another previously detected contour or another inside it?\\[4px]
					Calculated mathematically from vertex coordinate information. This is to filter out symbol (e.g. diamonds) and picture card misclassification. Previously detected contours in the list that turn out to be contained by newly detected contours are replaced by the newly detected and discarded.
				\item Is it significantly dissimilar to previously detected contours?\\[4px]
					The criteria for this check are that every rotation of the four contour vertices are at least a certain total distance away from their counterparts (summed up manhattan/city block distance in pixels). This minimum total distance is a function of the image size to normalise pixel units into units relative to the image dimensions. This relationship is \code{(image_size.width + image_size.height) * 0.5F * min_square_diff} or in other words, the mean image side length (to account for disproportionate image aspect ratios) multiplied by a constant factor set to $0.1$, i.e. 10\% of the mean image side length.
			\end{enumerate}

			Once all contours have been iterated through and the ones that fail these criteria have been filtered out, the result is a collection of contour quads that surround at least every card in the image (in the form of a \code{vector<vector<cv::Point> >} --- a vector of vectors with four points each). \emph{At least} every card in the image; the pruning process is not yet over!

			The assortment of contours go through a secondary, later filtration process that requires processing the pixel data within these contours. To do this --- as well as to construct the card Mats for subsequent classification --- the contour quad data is used to perspective transform the original image into a rectangular image of the card, an operation reviewed previously \secref{sec:isolation}.

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.6\linewidth]{perstrans1}
				\includegraphics[width=0.3\linewidth]{perstrans2}
				\caption{Output of Perspective Transform of Contour Quad from Test Image}
				\label{fig:perstrans}
			\end{figure}

			The perspective transform is low-level enough that its implementations do not vary much at all. The maths involved in calculating the 3x3 matrix of a perspective transform that can be used to map pixels at a perspective to other shapes/perspectives is standard but cumbersome to implement (unlike an affine transform). For these reasons, the OpenCV function \code{cv::getPerspectiveTransform()} was used. It takes two quads as inputs and returns the map matrix. The source quad is the detected contour and the destination quad is a simple 250x350 rectangle which matches the standard 5:7 poker card aspect ratio like in figure~\ref{fig:perstrans}.

			The transformation matrix is then used in the function \code{cv::warpPerspective()} which deforms the pixel grid of the source image and maps it to the destination image while interpolating transitional pixels; a process that is both non-trivial as well as distant from the task at hand. The Mat produced through this process is subsequently used to instantiate a new \code{Card} object.

			On instantiation, the \code{Card} constructor additionally generates an 8x8 CLAHE version and a 140 threshold binary version of the card Mat, which are publicly accessible from a \code{Card} object. Once instantiated, the cards binary Mat is finally used for the last phase of isolation filtering.

			The last criteria for what makes a card is its whiteness measure. The measure is simple; all non-zero (i.e. white) pixels in the card Mat are counted and the final value is divided by the card's area giving a float that describes the proportion of whiteness in the card. The whiteness threshold is set to an experimentally practical \code{0.5F} or 50\% bearing in mind that error due to lighting inconsistency is a non-issue following CLAHE. Cards that are not white enough are discarded. This solves false detections such as that in figure~\ref{fig:whiteness}.

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.9\linewidth]{whiteness}
				\caption{Example of Whiteness Check Ignoring Overturned Card}
				\label{fig:whiteness}
			\end{figure}

			Finally, there is one more processing step that needs to be undertaken before a detected card is deemed worthy enough to join its brethren in the detected card vector and \code{find_cards()} can return\ldots

		\subsubsection{Rotation Correction}

			One deal-breaking issue was the problem of a sequence of contour quad vertices starting at the wrong corner and, as a direct consequence, causing the final card Mat to be perspective transformed incorrectly. This problem became evident became evident when processing cards where the top-most corner in the image is numbered, such as the lower examples in figure~\ref{fig:rot}; the card could just be at a very strange perspective that projects to that shape.

			\begin{figure}[H]
				\centering
				\includegraphics[width=0.6\linewidth]{rot1}
				\includegraphics[width=0.3\linewidth]{rot2}\\[5px]
				\includegraphics[width=0.6\linewidth]{rot3}
				\includegraphics[width=0.3\linewidth]{rot4}
				\caption{Example of Erroneous Perspective Transformation}
				\label{fig:rot}
			\end{figure}

			The solution developed is simple and works very well. The same measure of whiteness as is used for the whole card is applied to both the top left and the bottom right corners of the card Mat (the bounds for which are defined as static constants in the \code{Card} class based on physical measurements). If corner whiteness surpasses 80\%, the card quad is rotated by one vertex --- the direction or rotation being arbitrary due to a card's symmetry --- and transformed into a new Mat.

			The corner whiteness for this new Mat is also measured, and compared against old corner whiteness. If it has a lesser corner whiteness measure than the old, that means that the old card Mat was indeed oriented incorrectly and so the new one is used instead. If the opposite holds true, then the first overall whiteness threshold was passed incorrectly (perhaps due to some bizarre specular lighting that the CLAHE did not quite catch) and so the new mat is discarded and the old one's usage is continued as if nothing ever happened.

			At long last, the \code{Card} vector contains all detected cards --- to a high degree of accuracy --- ready for processing and classification by the rest of the program, and the function can return.

	\subsection{Colour Detection}
		\input{colourdet.tex}

	\subsection{Type Detection}
	\subsection{Symbol/Rank symbol Isolation}
	\subsection{Number Detection}
		\input{numdet.tex}

	\subsection{Suit Symbol Detection}
		\input{suitdet.tex}

	\subsection{Results Output}
		\input{gui.tex}

\pagebreak
\section{Evaluation}
	\subsection{Test images, increasing complexity}
	\subsection{Webcam tests}
	\subsection{Confusion Matrix}
\pagebreak
\section{Conclusion}
	\subsection{Summary}
	\subsection{Improvements}
\pagebreak
\thispagestyle{empty}
\printbibliography
\pagebreak
\begin{appendices}
	\section{Contribution Agreement}
		\begin{figure}[H]
			\centering
			\includegraphics[width=\textwidth]{chris/image36}
		\end{figure}
	\pagebreak
	\section{Source Code Listings}
		\lstset{
			language=C++,
			showspaces=false,
			showtabs=false,
			breaklines=true,
			breakatwhitespace=true,
			escapeinside={(*@}{@*)},
			commentstyle=\color{greencomments},
			keywordstyle=\color{bluekeywords},
			stringstyle=\color{redstrings},
			basicstyle=\ttfamily\scriptsize,
			captionpos=t,
			extendedchars=true,
			frame=single,
			keepspaces=true,
			showstringspaces=false,
			stepnumber=2,
			tabsize=2,
		}

		\subsection{main.cpp}
		\label{app:maincpp}
		\lstinputlisting{res/appendices/src/main.cpp}
\end{appendices}

\vfill
\hrulefill

\end{document}

\section{Review}
\label{sec:review}

	\begin{equation*}
		T =
		\begin{bmatrix}
			84 & 200 & 16 \\
			56 & 238 & 11 \\
			251 & 206 & 203
		\end{bmatrix}
		\qquad
		I =
		\begin{bmatrix}
			160 & 40 & 250 & 27 & 114 \\
			81 & 94 & 14 & 100 & 37 \\
			225 & 52 & 148 & 137 & 111
		\end{bmatrix}
	\end{equation*}

	% Define block styles
	\tikzstyle{decision} = [diamond, draw, fill=blue!20, text badly centered, inner sep=0pt, text width=4em, font=\tiny]
	\tikzstyle{process} = [rectangle, draw, fill=blue!20, text centered, rounded corners, text width=3em, font=\tiny]
	\tikzstyle{line} = [draw, -latex', font=\tiny]
	\tikzstyle{cloud} = [draw, ellipse, fill=red!20, minimum height=2em]
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[node distance = 2cm, auto]
			% Place nodes
			\node [process] (imgget) {Get Image from File/Webcam};
			\node [process, right of=imgget] (cardfind) {Isolate Cards};
			\node [process, right of=cardfind] (colourdet) {Detect Colour};
			\node [process, right of=colourdet] (typedet) {Detect Type};
			\node [process, right of=typedet] (symfind) {Isolate Symbols};
			\node [decision, right of=symfind] (ispic) {Is Card Picture Type?};
			\node [process, above right of=ispic] (numdet) {Detect Number};
			\node [process, below right of=ispic] (rankdet) {Detect Rank};
			% Enhanced by colour detection
			\node [process, below right of=numdet] (symdet) {Detect Suit};


			% Draw edges
			\path [line] (imgget) -- (cardfind);
			\path [line] (cardfind) -- (colourdet);
			\path [line] (colourdet) -- (typedet);
			\path [line] (typedet) -- (symfind);
			\path [line] (symfind) -- (ispic);
			\path [line] (ispic) -- node {no} (numdet);
			\path [line] (ispic) -- node [below left] {yes} (rankdet);
			\path [line] (rankdet) -- (symdet);
			\path [line] (numdet) -- (symdet);
			%\path [line,dashed] (expert) -- (init);
			%\path [line,dashed] (system) -- (init);
			%\path [line,dashed] (system) |- (evaluate);
		\end{tikzpicture}
		\caption{Top-level Flowchart}
		\label{fig:flowchart}
	\end{figure}